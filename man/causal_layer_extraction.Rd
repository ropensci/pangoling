% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tr_causal.R
\name{causal_targets_layers}
\alias{causal_targets_layers}
\alias{causal_words_layers}
\alias{causal_tokens_layers}
\title{Extract hidden layer representations using a causal transformer model}
\usage{
causal_targets_layers(
  contexts,
  targets,
  sep = " ",
  layers = NULL,
  include_embeddings = TRUE,
  merge_fun = colMeans,
  return_type = c("list", "array"),
  model = getOption("pangoling.causal.default"),
  checkpoint = NULL,
  add_special_tokens = NULL,
  config_model = NULL,
  config_tokenizer = NULL,
  batch_size = 1
)

causal_words_layers(
  x,
  by = rep(1, length(x)),
  sep = " ",
  layers = NULL,
  include_embeddings = TRUE,
  merge_fun = colMeans,
  return_type = c("list", "array"),
  sorted = FALSE,
  model = getOption("pangoling.causal.default"),
  checkpoint = NULL,
  add_special_tokens = NULL,
  config_model = NULL,
  config_tokenizer = NULL,
  batch_size = 1
)

causal_tokens_layers(
  text,
  layers = NULL,
  include_embeddings = TRUE,
  merge_fun = NULL,
  return_type = c("list", "array"),
  model = getOption("pangoling.causal.default"),
  checkpoint = NULL,
  add_special_tokens = NULL,
  config_model = NULL,
  config_tokenizer = NULL
)
}
\arguments{
\item{contexts}{A character vector of context strings (for \code{causal_targets_layers}).}

\item{targets}{A character vector of target words/phrases (for \code{causal_targets_layers}).}

\item{sep}{A string specifying how words are separated within contexts or
groups. Default is \code{" "}. For languages that don't have spaces
between words (e.g., Chinese), set \code{sep = ""}.}

\item{layers}{Integer vector specifying which layers to extract. Use \code{NULL}
(default) to extract all layers. Layer 0 is the non-contextualized embeddings.}

\item{merge_fun}{Function to merge multi-token words into single representations.
Default is \code{colMeans} which averages across tokens for each dimension.
The function should take a matrix \link{n_tokens, hidden_dim} and return
a vector of length hidden_dim.
Other options: \code{function(x) x[1,]} (use first token only),
\code{colSums} (sum across tokens), or any custom function.
Set to \code{NULL} to disable merging and return all tokens separately
(output will be a matrix \link{n_tokens, hidden_dim}).}

\item{return_type}{Either "list" (default) or "array". If "list", returns a
named list with one element per layer. If "array", returns a 3D array with
dimensions \link{layers, tokens, hidden_size}.}

\item{model}{Name of a pre-trained model or folder. One should be able to use
models based on "gpt2". See
\href{https://huggingface.co/models?other=gpt2}{hugging face website}.}

\item{checkpoint}{Folder of a checkpoint.}

\item{add_special_tokens}{Whether to include special tokens. It has the
same default as the
\href{https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/auto#transformers.AutoTokenizer}{AutoTokenizer}
method in Python.}

\item{config_model}{List with other arguments that control how the
model from Hugging Face is accessed.}

\item{config_tokenizer}{List with other arguments that control how the
tokenizer from Hugging Face is accessed.}

\item{batch_size}{Maximum number of sentences/texts processed in parallel.
Larger batches increase speed but use more memory. Since
all texts in a batch must have the same length, shorter
ones are padded with placeholder tokens.}

\item{x}{A character vector of words (for \code{causal_words_layers}) or a single
string (for \code{causal_tokens_layers}).}

\item{by}{A vector for grouping elements of \code{x} (for \code{causal_words_layers}).}

\item{text}{A single string (for \code{causal_tokens_layers}).}
}
\value{
A named list or 3D array of hidden states. Structure depends on
\code{return_type}:
\itemize{
\item If "list": Named list where each element is a matrix \link{tokens × hidden_size}
\item If "array": 3D array \link{layers × tokens × hidden_size}
}

For \code{causal_words_layers}, returns a list of such structures (one per group).
}
\description{
These functions extract hidden layer representations (embeddings) from
transformer models for words, phrases, or tokens.
}
\details{
These functions extract hidden states from all layers of a transformer model,
including layer 0 (non-contextualized token embeddings) and layers 1-N
(contextualized representations from each transformer block).

\strong{Layer numbering:}
\itemize{
\item Layer 0: Non-contextualized token embeddings (input to the transformer)
\item Layers 1-N: Output from each transformer block (contextualized)
\item \strong{\code{causal_targets_layers()}}: Extract layers for specific target words or
phrases based on their contexts. Returns hidden states for each target token.
\item \strong{\code{causal_words_layers()}}: Extract layers for all words in grouped text
(e.g., sentences or paragraphs).
\item \strong{\code{causal_tokens_layers()}}: Extract layers for all tokens in a single text.
}
}
\examples{
\dontrun{
# First, preload with output_hidden_states = TRUE
causal_preload(model = "gpt2", output_hidden_states = TRUE)

# Extract layers for specific targets
layers <- causal_targets_layers(
  contexts = c("The cat sat on the", "The dog ran in the"),
  targets = c("mat", "park"),
  model = "gpt2"
)

# Extract only layer 0 (embeddings) and layer 12 (final layer)
layers_subset <- causal_targets_layers(
  contexts = "The apple fell from the",
  targets = "tree",
  layers = c(0, 12),
  model = "gpt2"
)

# Get as array instead of list
layers_array <- causal_targets_layers(
  contexts = "Once upon a",
  targets = "time",
  return_type = "array",
  model = "gpt2"
)
dim(layers_array)  # [n_layers, n_tokens, hidden_size]
}

}
\seealso{
Other causal model functions: 
\code{\link{causal_next_tokens_pred_tbl}()},
\code{\link{causal_pred_mats}()},
\code{\link{causal_words_pred}()}
}
\concept{causal model functions}
